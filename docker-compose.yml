services:
  # Base de datos para MLflow
  mysql:
    image: mysql:5.7
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: mlflow
    volumes:
      - mysql_data:/var/lib/mysql
    ports:
      - "3306:3306"
    restart: on-failure

  # Base de datos para Airflow
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    restart: on-failure

  # Redis para CeleryExecutor
  redis:
    image: redis:latest
    container_name: redis
    expose:
      - "6379"
    restart: on-failure

  # InicializaciÃ³n de Airflow
  airflow-init:
    image: apache/airflow:2.7.0-python3.9
    container_name: airflow-init
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
    entrypoint: >
      /bin/bash -c "
      airflow db init &&
      airflow db upgrade &&
      airflow users create --username airflow --password airflow --firstname Admin --lastname User --role Admin --email admin@example.com
      "
    restart: "no"

  # Airflow Webserver
  airflow-webserver:
    image: apache/airflow:2.7.0-python3.9
    container_name: airflow-webserver
    command: webserver
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      _PIP_ADDITIONAL_REQUIREMENTS: "pymysql requests pandas scikit-learn mlflow"
      AIRFLOW__LOGGING__REMOTE_LOGGING: "False"
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./datos_raw:/opt/airflow/datos_raw
      - ./modelos:/opt/airflow/modelos

    restart: on-failure

  # Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.7.0-python3.9
    container_name: airflow-scheduler
    command: scheduler
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      _PIP_ADDITIONAL_REQUIREMENTS: "pymysql requests pandas scikit-learn mlflow"
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AIRFLOW__LOGGING__REMOTE_LOGGING: "False"

    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./datos_raw:/opt/airflow/datos_raw
      - ./modelos:/opt/airflow/modelos

    restart: on-failure

  # Airflow Worker
  airflow-worker:
    image: apache/airflow:2.7.0-python3.9
    container_name: airflow-worker
    command: celery worker
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      _PIP_ADDITIONAL_REQUIREMENTS: "pymysql requests pandas scikit-learn mlflow"
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AIRFLOW__LOGGING__REMOTE_LOGGING: "False"


    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./datos_raw:/opt/airflow/datos_raw
      - ./modelos:/opt/airflow/modelos

    restart: on-failure

  # MinIO para almacenamiento de artefactos
  minio:
    image: minio/minio
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    volumes:
      - minio_data:/data
    ports:
      - "9000:9000"
      - "9001:9001"
    restart: on-failure

  # MLflow tracking + model registry
  mlflow:
    image: python:3.10
    container_name: mlflow
    working_dir: /mlflow
    volumes:
      - ./mlflow:/mlflow
    command: >
      bash -c "
        pip install mlflow pymysql boto3 &&
        mlflow server --backend-store-uri mysql+pymysql://root:root@mysql/mlflow --default-artifact-root s3://mlflow/ --host 0.0.0.0 --port 5000
      "
    environment:
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
    depends_on:
      - mysql
      - minio
    ports:
      - "5000:5000"
    restart: on-failure

  api-inferencia:
    build:
      context: .
      dockerfile: Dockerfile  # o ajusta si lo pusiste en otra carpeta
    container_name: api-inferencia
    ports:
      - "8000:8000"
    volumes:
      - ./modelos:/app/modelos
    restart: on-failure
  
  streamlit-ui:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    ports:
      - "8503:8503"
    volumes:
      - ./modelos:/app/modelos
    depends_on:
    - api-inferencia

volumes:
  airflow_logs:
  airflow_plugins:
  mysql_data:
  postgres-db-volume:
  minio_data:

